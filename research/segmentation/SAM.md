- generate masks automatically
- create segmentation masks using bounding boxes
- convert object detection datasets into segmentation masks

three different encodes you can use (from less to more params, fast to slow, lower quality to higher quality):
ViT-B, ViT-L, and ViT-H.

### Generating masks

So, you can generate masks of basically everything in an image. Every mask/segmentation have the following fields:
segmentation - [np.ndarray]:
the mask with (W, H) shape, and bool type, where W and H are the width and height of the original image, respectively

area - [int]:
the area of the mask in pixels

bbox - [List[int]]:
the boundary box detection in xywh format

predicted_iou - [float]:
the model's own prediction for the quality of the mask

point_coords - [List[List[float]]]:
the sampled input point that generated this mask

stability_score - [float]:
an additional measure of mask quality

crop_box - List[int]:
the crop of the image used to generate this mask in xywh format

### Generating segmentation with bounding box

- from a bounding box, generate the mask

## Notes

- can basically segment all skin -> possible to replace model with different skin colour
- can be prompted with text, bounding box, pointer and mask

## nb: Using SAM with prompts

predictor.set_image: The model first converts the image into an image embedding.
predictor.predict:

```py
masks, scores, logits = predictor.predict(
    point_coords=input_point,
    point_labels=input_label,
    multimask_output=True,
)
```

With the above settings, we get 3 masks:
scores: the model's estimation of the quality of the masks

**with points**:
"works" when utilizing background points. put background point on skin, and other points on the clothes.

**with bounding boxes**:
Better.

**with bounding boxes and points**:
Very good.

All in all, by combining object detection (to get bounding boxes) and SAM. It should be possible to segment all clothes from an image.

## nb: Automatically generating masks

Since SAM can efficiently process prompts, masks for the entire image can be generated by sampling a large number of prompts over an image.

SamAutomaticMaskGenerator implements this capability by sampling single-point input prompts in a grid over the image (SAM predicts a mask from each one of them). Masks are then filtered for quality and stuff. Seems like we can do some postprocessing here? "post-processing can remove stray pixels and holes".

### Hardware

From Meta's web demo we can see that:

- The computation of embeddings is done server side (probably with a GPU)
- The mask predictions are then done client side

### Conclusion
