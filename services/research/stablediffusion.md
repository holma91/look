### pipeline flow

the unet predicts the noise residual, and the scheduler uses that to predict a less noisy image. repeat until number of inference steps is reached.

Stable Diffusion is called a latent diffusion model because it works with a lower-dimensional representation of the image (the encoded image) instead of the actual pixel space

tokenizer and encoder are needed to generate text embeddings.

why do we need a prompt?
we tokenize text to generate embeddings. the text is used to condition the UNet model and steer the diffusion process towards something that resembles the input prompt. every token has a related embedding, with pixels that looks like what the token describes.

we get the text_embeddings by running the prompt through the text_encoder.

unconditional text embeddings: embeddings for the padding token

the latent representation of an image: the initial random noise

we create a denoising loop that progressively transforms the pure noise in the latents to an image described by the prompt:

1. Set the scheduler's timesteps to use during denoising.
2. Iterate over the timesteps.
3. At each timestep, call the UNet model to predict the noise residual and pass it to the scheduler to compute the previous noisy sample.

We use the vae to decode the latent representation into an image.

### how does stable diffusion work?

Generally speaking, diffusion models are machine learning systems that are trained to denoise random Gaussian noise step by step, to get to a sample of interest, such as an image

the key difference between standard diffusion and latent diffusion:

- latent diffusion apply the diffusion process over a lower dimensional latent space (instead of the actual pixel space)

the three main components in latent diffusion are:

1. the autoencoder (VAE)
   has two parts, the encoder and the decoder.
   During training, the encoder is used to get the latents of the images for the fwd diffusion process, which applies more and more noise at each step. ONLY the encoder is needed during training.
   During inference, the denoised latents generated by the reverse diffusion process are converted back into images using the decoder. ONLY the decoder is needed during inference.
2. the U-Net
   has an encoder part and a decoder part (?), both comprised of ResNet blocks. the U-Net predicts the noise residual.
3. the text-encoder (e.g CLIP's text encoder or BERT)
   the text-encoder is responsible for transforming the input prompt into an embedding space that can be understood by the U-Net. It usually just maps a sequence of input tokens to a sequence of latent text-embeddings. Basically it associates words with pixels that somehow looks like what the word is describing. CLIP.

token embeddings

- every single token mapped to a n-dimensional vector
- the n-dimensional vector should somehow describe what the token looks like

positional embeddings

- tell the model where in a sequence a token is

timesteps?
can be used to create a noise schedule that tells you what the noise is at a "timestep". as the timestep increases, the noise goes down?

unconditional image generation: the task of generating images with no condition in any context (like a prompt text or another image)

**controlled generation**

### Train a diffusion model

Typically, the best results are obtained from finetuning a pretrained model on a specific dataset.

### Textual inversion

very bad results on first try

### Dreambooth

**in depth**

- https://huggingface.co/blog/dreambooth
- https://arxiv.org/abs/2208.12242

hard to select the right hyperparams, and easy to overfit.

- find a sweet spot between the number of training steps and learning rate
  - hf recommends low learning rate and progressively increasing the number of steps until satisfactory results
  - needs more training steps for faces
    example given is 800-1200 steps with a batch size of 2 and lr of 1e-6
- prior preservation is important to avoid overfitting when training on faces.
- noisy or bad quality images -> you are overfitting.
  - use the DDIM scheduler or run more inference steps to fix.
- training the text encoder in addition to the UNet has a big impact on quality
  - best results came from a combination of text encoder fine-tuning, low LR and a suitable number of steps.
- fine-tuning with or without EMA produced similar results.

- when the model overfits, DDIM usually works much better than PNDM and LMSDiscrete
- prior preservations tries to reduce overfitting by using photos of the new person combined with photos of other people

textual inversion + dreambooth?

### sd-training-intro

Concepts falls under two categories:

- subjects
- styles

No hard limit to number of concept you can add, but more concepts make it harder (and slower) to train.
Models don't grow in size during training, so when we add concepts the models previous knowledge gets harmed.

- prevent this to a certain point by training on other things too (regularization/class data)

**full fine-tuning**
Aims at training the whole model on every concept it can. Requires an enormous dataset to do correctly (?).

A dataset is composed of:

- instance data (pictures of what you want to train)
- regularization data or Class data (pictures of diverse other things)

training on single subject concept: 5-20 pictures
training on a style concept: 50-200 pictures
full fine-tuning (Everydream): train on multiple thousands pictures. each concept is treated like a single concept would.

each picture has a caption, there are different approaches to captioning:

- single token caption
- multi token caption
- full sentence caption
  - mostly useful in full fine-tuning

### misc

(dreambooth OR everydream OR lora OR fine-tuning) (from:dannypostmaa or levelsio)

**levelsio**
cost per dreambooth fine-tune:

- $0.50-$3 per fine-tune
  is levelsio fine-tuning with everydream?
- a custom model called people-diffusion
- "You should train directly on Replicate with the Dreambooth trainer": https://replicate.com/replicate/dreambooth

**dannypostmaa**

- uses dreambooth with controlnet
- DALL-E inpainting?
- ESRGAN upscaling?
- automask eye and inpaint it?
- auto segmentation -> apply mask -> inpainting

from danny; the process of training:
Still trying out the details myself, but comes down to:

1. Heavily tuning DreamBooth parameters
2. Trying different class_data sets
3. DAYS worth of prompt testing.
